{
 "cells": [
  {
   "cell_type": "code",
   "id": "5c514add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T15:05:15.465305Z",
     "start_time": "2024-11-01T15:04:32.043648Z"
    }
   },
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_parquet(\"hf://datasets/YaYaB/onepiece-blip-captions/data/train-00000-of-00001-7e86e8a67581ad82.parquet\")\n",
    "\n",
    "# Directory to save images\n",
    "output_folder = \"images2\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over rows to save each image\n",
    "for idx, row in df.iterrows():\n",
    "    # Assuming 'image' column contains image data in base64 format or as bytes\n",
    "    image_data = row['image'].get('bytes')  # Modify if necessary based on actual format (e.g., if it's already binary)\n",
    "    \n",
    "    # Decode if base64 encoded (skip this step if images are already in bytes format)\n",
    "    if isinstance(image_data, str):  \n",
    "        image_data = base64.b64decode(image_data)\n",
    "    \n",
    "    # Convert bytes data to an Image object\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    # Save image to output folder with a unique name\n",
    "    image_filename = os.path.join(output_folder, f\"image_{idx}.png\")\n",
    "    image.save(image_filename)\n",
    "\n",
    "'''\n",
    "Use One piece to train a Gaussian Diffusion model\n",
    "Figure out how to get the images onto your computer.\n",
    "Learn the architecture and minimize the source file to only the essential pieces of code.\n",
    "*Try using local PC. The original DDPM was only 34M parameters\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse One piece to train a Gaussian Diffusion model\\nFigure out how to get the images onto your computer.\\nLearn the architecture and minimize the source file to only the essential pieces of code.\\n*Try using local PC. The original DDPM was only 34M parameters\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "adcaeb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T19:19:49.792283Z",
     "start_time": "2024-10-31T19:18:59.869692Z"
    }
   },
   "source": [
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000    # number of steps\n",
    ")\n",
    "\n",
    "training_images = torch.rand(8, 3, 128, 128) # images are normalized from 0 to 1\n",
    "loss = diffusion(training_images)\n",
    "loss.backward()\n",
    "\n",
    "# after a lot of training\n",
    "\n",
    "sampled_images = diffusion.sample(batch_size = 4)\n",
    "sampled_images.shape # (4, 3, 128, 128)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\backends\\cuda\\__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de2c77c828154d63b67404ae4f8bdf90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 19\u001B[0m\n\u001B[0;32m     15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# after a lot of training\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m sampled_images \u001B[38;5;241m=\u001B[39m diffusion\u001B[38;5;241m.\u001B[39msample(batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m     20\u001B[0m sampled_images\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:747\u001B[0m, in \u001B[0;36mGaussianDiffusion.sample\u001B[1;34m(self, batch_size, return_all_timesteps)\u001B[0m\n\u001B[0;32m    745\u001B[0m (h, w), channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannels\n\u001B[0;32m    746\u001B[0m sample_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_ddim_sampling \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mddim_sample\n\u001B[1;32m--> 747\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sample_fn((batch_size, channels, h, w), return_all_timesteps \u001B[38;5;241m=\u001B[39m return_all_timesteps)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:693\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample_loop\u001B[1;34m(self, shape, return_all_timesteps)\u001B[0m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps)), desc \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msampling loop time step\u001B[39m\u001B[38;5;124m'\u001B[39m, total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps):\n\u001B[0;32m    692\u001B[0m     self_cond \u001B[38;5;241m=\u001B[39m x_start \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_condition \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 693\u001B[0m     img, x_start \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample(img, t, self_cond)\n\u001B[0;32m    694\u001B[0m     imgs\u001B[38;5;241m.\u001B[39mappend(img)\n\u001B[0;32m    696\u001B[0m ret \u001B[38;5;241m=\u001B[39m img \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_all_timesteps \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(imgs, dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:677\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample\u001B[1;34m(self, x, t, x_self_cond)\u001B[0m\n\u001B[0;32m    675\u001B[0m b, \u001B[38;5;241m*\u001B[39m_, device \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m*\u001B[39mx\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m    676\u001B[0m batched_times \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((b,), t, device \u001B[38;5;241m=\u001B[39m device, dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m--> 677\u001B[0m model_mean, _, model_log_variance, x_start \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_mean_variance(x \u001B[38;5;241m=\u001B[39m x, t \u001B[38;5;241m=\u001B[39m batched_times, x_self_cond \u001B[38;5;241m=\u001B[39m x_self_cond, clip_denoised \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    678\u001B[0m noise \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn_like(x) \u001B[38;5;28;01mif\u001B[39;00m t \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0.\u001B[39m \u001B[38;5;66;03m# no noise if t == 0\u001B[39;00m\n\u001B[0;32m    679\u001B[0m pred_img \u001B[38;5;241m=\u001B[39m model_mean \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m model_log_variance)\u001B[38;5;241m.\u001B[39mexp() \u001B[38;5;241m*\u001B[39m noise\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:664\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_mean_variance\u001B[1;34m(self, x, t, x_self_cond, clip_denoised)\u001B[0m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp_mean_variance\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t, x_self_cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, clip_denoised \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 664\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_predictions(x, t, x_self_cond)\n\u001B[0;32m    665\u001B[0m     x_start \u001B[38;5;241m=\u001B[39m preds\u001B[38;5;241m.\u001B[39mpred_x_start\n\u001B[0;32m    667\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clip_denoised:\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:639\u001B[0m, in \u001B[0;36mGaussianDiffusion.model_predictions\u001B[1;34m(self, x, t, x_self_cond, clip_x_start, rederive_pred_noise)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodel_predictions\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t, x_self_cond \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, clip_x_start \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, rederive_pred_noise \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 639\u001B[0m     model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(x, t, x_self_cond)\n\u001B[0;32m    640\u001B[0m     maybe_clip \u001B[38;5;241m=\u001B[39m partial(torch\u001B[38;5;241m.\u001B[39mclamp, \u001B[38;5;28mmin\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1.\u001B[39m, \u001B[38;5;28mmax\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m clip_x_start \u001B[38;5;28;01melse\u001B[39;00m identity\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpred_noise\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:410\u001B[0m, in \u001B[0;36mUnet.forward\u001B[1;34m(self, x, time, x_self_cond)\u001B[0m\n\u001B[0;32m    407\u001B[0m x \u001B[38;5;241m=\u001B[39m block1(x, t)\n\u001B[0;32m    408\u001B[0m h\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[1;32m--> 410\u001B[0m x \u001B[38;5;241m=\u001B[39m block2(x, t)\n\u001B[0;32m    411\u001B[0m x \u001B[38;5;241m=\u001B[39m attn(x) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m    412\u001B[0m h\u001B[38;5;241m.\u001B[39mappend(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:189\u001B[0m, in \u001B[0;36mResnetBlock.forward\u001B[1;34m(self, x, time_emb)\u001B[0m\n\u001B[0;32m    185\u001B[0m     scale_shift \u001B[38;5;241m=\u001B[39m time_emb\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m2\u001B[39m, dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    187\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock1(x, scale_shift \u001B[38;5;241m=\u001B[39m scale_shift)\n\u001B[1;32m--> 189\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock2(h)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m h \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mres_conv(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:157\u001B[0m, in \u001B[0;36mBlock.forward\u001B[1;34m(self, x, scale_shift)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, scale_shift \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 157\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(x)\n\u001B[0;32m    158\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m exists(scale_shift):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    457\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T15:08:01.800254Z",
     "start_time": "2024-11-01T15:07:53.176935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_parquet(\"hf://datasets/YaYaB/onepiece-blip-captions/data/train-00000-of-00001-7e86e8a67581ad82.parquet\")\n",
    "\n",
    "# Directories to save images\n",
    "original_output_folder = \"images_folder\"\n",
    "resized_output_folder = \"images128\"\n",
    "os.makedirs(original_output_folder, exist_ok=True)\n",
    "os.makedirs(resized_output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over rows to process each image\n",
    "for idx, row in df.iterrows():\n",
    "    # Access the image bytes\n",
    "    image_data = row['image'].get('bytes')\n",
    "    \n",
    "    # Open the image from bytes\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    # Resize image, maintaining aspect ratio, then pad to 128x128 if needed\n",
    "    image.thumbnail((128, 128), Image.LANCZOS)  # Resize with aspect ratio\n",
    "    \n",
    "    # Pad the image to make it exactly 128x128\n",
    "    image_padded = ImageOps.pad(image, (128, 128), color=(255, 255, 255))  # White padding, adjust color as needed\n",
    "    \n",
    "    # Save original and resized images\n",
    "    original_filename = os.path.join(original_output_folder, f\"image_{idx}.png\")\n",
    "    resized_filename = os.path.join(resized_output_folder, f\"image_{idx}.png\")\n",
    "    image.save(original_filename)\n",
    "    image_padded.save(resized_filename)\n"
   ],
   "id": "ec0f4c59821c8961",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bf4900b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T15:15:48.852445Z",
     "start_time": "2024-11-01T15:14:34.976590Z"
    }
   },
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    'images128',\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 700000,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    "    calculate_fid = True              # whether to calculate fid during training\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Model checkpoint saved to {checkpoint_path}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\miniconda3\\envs\\test2\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to C:\\Users\\jesse/.cache\\torch\\hub\\checkpoints\\pt_inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:04<00:00, 20.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/700000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a431cd4e12e4b8993f6a136a09662d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\backends\\cuda\\__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jesse\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\attend.py:88: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  out = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 130.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 28\u001B[0m\n\u001B[0;32m      9\u001B[0m diffusion \u001B[38;5;241m=\u001B[39m GaussianDiffusion(\n\u001B[0;32m     10\u001B[0m     model,\n\u001B[0;32m     11\u001B[0m     image_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m,\n\u001B[0;32m     12\u001B[0m     timesteps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m,           \u001B[38;5;66;03m# number of steps\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     sampling_timesteps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m250\u001B[39m    \u001B[38;5;66;03m# number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\u001B[39;00m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     16\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     17\u001B[0m     diffusion,\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages128\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     calculate_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m              \u001B[38;5;66;03m# whether to calculate fid during training\u001B[39;00m\n\u001B[0;32m     26\u001B[0m )\n\u001B[1;32m---> 28\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     29\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoint.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     30\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), checkpoint_path)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:1058\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1055\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdl)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m   1057\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mautocast():\n\u001B[1;32m-> 1058\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(data)\n\u001B[0;32m   1059\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulate_every\n\u001B[0;32m   1060\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\accelerate\\utils\\operations.py:819\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 819\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\accelerate\\utils\\operations.py:807\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    806\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 807\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[1;32m---> 16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:841\u001B[0m, in \u001B[0;36mGaussianDiffusion.forward\u001B[1;34m(self, img, *args, **kwargs)\u001B[0m\n\u001B[0;32m    838\u001B[0m t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps, (b,), device\u001B[38;5;241m=\u001B[39mdevice)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[0;32m    840\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize(img)\n\u001B[1;32m--> 841\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_losses(img, t, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:817\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_losses\u001B[1;34m(self, x_start, t, noise, offset_noise_strength)\u001B[0m\n\u001B[0;32m    813\u001B[0m         x_self_cond\u001B[38;5;241m.\u001B[39mdetach_()\n\u001B[0;32m    815\u001B[0m \u001B[38;5;66;03m# predict and take gradient step\u001B[39;00m\n\u001B[1;32m--> 817\u001B[0m model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(x, t, x_self_cond)\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpred_noise\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    820\u001B[0m     target \u001B[38;5;241m=\u001B[39m noise\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:426\u001B[0m, in \u001B[0;36mUnet.forward\u001B[1;34m(self, x, time, x_self_cond)\u001B[0m\n\u001B[0;32m    424\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, h\u001B[38;5;241m.\u001B[39mpop()), dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    425\u001B[0m     x \u001B[38;5;241m=\u001B[39m block2(x, t)\n\u001B[1;32m--> 426\u001B[0m     x \u001B[38;5;241m=\u001B[39m attn(x) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m    428\u001B[0m     x \u001B[38;5;241m=\u001B[39m upsample(x)\n\u001B[0;32m    430\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, r), dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\Independent Study\\Code\\denoising-diffusion-pytorch\\denoising_diffusion_pytorch\\denoising_diffusion_pytorch.py:232\u001B[0m, in \u001B[0;36mLinearAttention.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    228\u001B[0m k \u001B[38;5;241m=\u001B[39m k\u001B[38;5;241m.\u001B[39msoftmax(dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    230\u001B[0m q \u001B[38;5;241m=\u001B[39m q \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale\n\u001B[1;32m--> 232\u001B[0m context \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb h d n, b h e n -> b h d e\u001B[39m\u001B[38;5;124m'\u001B[39m, k, v)\n\u001B[0;32m    234\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb h d e, b h d n -> b h e n\u001B[39m\u001B[38;5;124m'\u001B[39m, context, q)\n\u001B[0;32m    235\u001B[0m out \u001B[38;5;241m=\u001B[39m rearrange(out, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb h c (x y) -> b (h c) x y\u001B[39m\u001B[38;5;124m'\u001B[39m, h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads, x \u001B[38;5;241m=\u001B[39m h, y \u001B[38;5;241m=\u001B[39m w)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\functional.py:385\u001B[0m, in \u001B[0;36meinsum\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m einsum(equation, \u001B[38;5;241m*\u001B[39m_operands)\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(operands) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39menabled:\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001B[39;00m\n\u001B[0;32m    384\u001B[0m     \u001B[38;5;66;03m# or the user has disabled using opt_einsum\u001B[39;00m\n\u001B[1;32m--> 385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39meinsum(equation, operands)  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    387\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39mis_available():\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 130.00 MiB. GPU "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T15:14:22.165304Z",
     "start_time": "2024-11-01T15:14:16.249843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "\n",
    "# Define the model architecture (same as in training)\n",
    "model = Unet(\n",
    "    dim=64,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    flash_attn=True\n",
    ")\n",
    "\n",
    "# Define the diffusion process\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=128,\n",
    "    timesteps=1000,\n",
    "    sampling_timesteps=250  # using DDIM sampling for faster inference\n",
    ")\n",
    "\n",
    "# Load the trained model checkpoint (replace with your checkpoint path)\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n",
    "model.eval()  # Set the model to evaluation mode"
   ],
   "id": "8c328a13bd590105",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Load the trained model checkpoint (replace with your checkpoint path)\u001B[39;00m\n\u001B[0;32m     20\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoint.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 21\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(checkpoint_path, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     22\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\serialization.py:997\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    995\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 997\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    998\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    999\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\serialization.py:444\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\serialization.py:425\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 425\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'checkpoint.pth'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Directory to save generated samples\n",
    "output_folder = \"generated_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate a batch of samples\n",
    "with torch.no_grad():\n",
    "    sampled_images = diffusion.sample(batch_size=4)  # (4, 3, 128, 128) tensor\n",
    "\n",
    "# Save generated images\n",
    "for i, img_tensor in enumerate(sampled_images):\n",
    "    img_array = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(\"uint8\")  # Convert to image format\n",
    "    img = Image.fromarray(img_array)\n",
    "    img.save(os.path.join(output_folder, f\"generated_image_{i}.png\"))\n"
   ],
   "id": "bcdb35bafe8e7daa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
